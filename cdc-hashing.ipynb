{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85aa6d7",
   "metadata": {},
   "source": [
    "## Hashing the rows from incoming and existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceaef474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/07/09 09:28:02 WARN Utils: Your hostname, rohitkarki resolves to a loopback address: 127.0.1.1; using 10.13.163.99 instead (on interface wlp4s0)\n",
      "25/07/09 09:28:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/rohitkarki/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/rohitkarki/.ivy2/cache\n",
      "The jars for the packages stored in: /home/rohitkarki/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.3_2.12 added as a dependency\n",
      "mysql#mysql-connector-java added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-490e8767-99c4-40a5-952d-316d3fd814e1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.5.2 in central\n",
      "\tfound mysql#mysql-connector-java;8.0.33 in central\n",
      "\tfound com.mysql#mysql-connector-j;8.0.33 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 310ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;8.0.33 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.33 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.5.2 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-490e8767-99c4-40a5-952d-316d3fd814e1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/8ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/07/09 09:28:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"IcebergWithPySpark\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.5.2,\"\n",
    "        \"mysql:mysql-connector-java:8.0.33,\"\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    )\n",
    "    .config(\"spark.sql.catalog.mysql\", \"org.apache.iceberg.spark.SparkCatalog\")    \n",
    "    .config(\"spark.sql.catalog.mysql.type\", \"jdbc\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.mysql.uri\",\n",
    "        \"jdbc:mysql://localhost:3306/exampledb\",\n",
    "    )\n",
    "    .config(\"spark.sql.catalog.mysql.jdbc.user\", \"exampleuser\")\n",
    "    .config(\"spark.sql.catalog.mysql.jdbc.password\", \"examplepass\")\n",
    "    .config(\"spark.sql.catalog.mysql.jdbc.driver\", \"com.mysql.cj.jdbc.Driver\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.mysql.warehouse\",\n",
    "        \"hdfs://namenode:9000/user/hive/warehouse\",\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8717da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n",
      "25/07/09 09:28:11 WARN JdbcCatalog: JDBC catalog is initialized without view support. To auto-migrate the database's schema and enable view support, set jdbc.schema-version=V1\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default|customer_dim|      false|\n",
      "|  default|       hello|      false|\n",
      "|  default|      movie_|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check current catalog and namespace\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS mysql.default\").show()\n",
    "\n",
    "\n",
    "spark.sql(\"SHOW NAMESPACES IN mysql\").show()\n",
    "\n",
    "# If sales namespace exists, try to use it\n",
    "spark.sql(\"USE mysql.default\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c04cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, sha2, concat_ws, col, lit\n",
    "\n",
    "def add_hash_column(df, columns_to_hash, hash_type=\"md5\"):\n",
    "    \"\"\"Add hash column to DataFrame\"\"\"\n",
    "    if hash_type.lower() == \"md5\":\n",
    "        return df.withColumn(\"row_hash\", md5(concat_ws(\"|\", *columns_to_hash)))\n",
    "    else:\n",
    "        return df.withColumn(\"row_hash\", sha2(concat_ws(\"|\", *columns_to_hash), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd389c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+\n",
      "| id| data|category|\n",
      "+---+-----+--------+\n",
      "|  1|hello|       A|\n",
      "|  2|world|       B|\n",
      "|  3|  foo|       A|\n",
      "+---+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM hello\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+\n",
      "| id| data|category|\n",
      "+---+-----+--------+\n",
      "|  1|hello|       A|\n",
      "|  2|world|       B|\n",
      "|  3|  foo|       A|\n",
      "+---+-----+--------+\n",
      "\n",
      "+---+-----+--------+\n",
      "| id| data|category|\n",
      "+---+-----+--------+\n",
      "|  1|  Lol|       A|\n",
      "|  4|hello|       B|\n",
      "+---+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "incoming_data = [\n",
    "    (1, \"Lol\", \"A\"),  # changed \n",
    "    (4, \"hello\", \"B\"), # new\n",
    "]\n",
    "\n",
    "existing_df = spark.read.table(\"mysql.default.hello\")\n",
    "existing_df.show()\n",
    "\n",
    "\n",
    "new_df = spark.createDataFrame(incoming_data, [\n",
    "    \"id\", \"data\", \"category\"\n",
    "])\n",
    "new_df.show()\n",
    "\n",
    "# Columns to include in hash (exclude metadata columns)\n",
    "columns_to_hash = [col for col in existing_df.columns if col not in [\"row_hash\", \"updated_at\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd32ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DATAS\n",
      "+---+----+--------+--------------------+\n",
      "| id|data|category|            row_hash|\n",
      "+---+----+--------+--------------------+\n",
      "|  1| Lol|       A|b310add6c3e16f67b...|\n",
      "+---+----+--------+--------------------+\n",
      "\n",
      "Changed DATAS\n",
      "+---+-----+--------+--------------------+\n",
      "| id| data|category|            row_hash|\n",
      "+---+-----+--------+--------------------+\n",
      "|  4|hello|       B|ee6f2af077718ff5c...|\n",
      "+---+-----+--------+--------------------+\n",
      "\n",
      "+---+-----+--------+--------------------+\n",
      "| id| data|category|            row_hash|\n",
      "+---+-----+--------+--------------------+\n",
      "|  1|  Lol|       A|b310add6c3e16f67b...|\n",
      "|  4|hello|       B|ee6f2af077718ff5c...|\n",
      "+---+-----+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import sha2, concat_ws, col, lit\n",
    "\n",
    "# def perform_cdc(existing_df, new_df, primary_key=\"id\"):\n",
    "#     # Read data\n",
    "#     # perform reading from Iceberg table\n",
    "\n",
    "#     # Get columns to hash (exclude metadata columns)\n",
    "#     hash_columns = [c for c in existing_df.columns if c not in [primary_key, \"row_hash\", \"updated_at\", \"active\"]]\n",
    "    \n",
    "#     # Add hash columns\n",
    "#     target_hashed = existing_df.withColumn(\"row_hash\", sha2(concat_ws(\"|\", *hash_columns), 256))\n",
    "#     source_hashed = new_df.withColumn(\"row_hash\", sha2(concat_ws(\"|\", *hash_columns), 256))\n",
    "    \n",
    "#     # Find new\n",
    "#     new_rows = source_hashed.join(\n",
    "#         target_hashed.select(primary_key, \"row_hash\"),\n",
    "#         [primary_key],\n",
    "#         \"left_anti\"\n",
    "#     )\n",
    "#     print(\"New DATAS\")\n",
    "#     new_rows.show()\n",
    "#     # Detect changed rows: same ID, but different row_hash\n",
    "#     source_alias = source_hashed.alias(\"src\")\n",
    "#     target_alias = target_hashed.alias(\"tgt\")\n",
    "\n",
    "#     changed_rows = source_alias.join(\n",
    "#         target_alias.select(primary_key, \"row_hash\"),\n",
    "#         on=primary_key,\n",
    "#         how=\"inner\"\n",
    "#     ).filter(\n",
    "#         col(\"src.row_hash\") != col(\"tgt.row_hash\")\n",
    "#     ).select(\"src.*\")\n",
    "    \n",
    "#     print(\"Changed DATAS\")\n",
    "#     changed_rows.show()\n",
    "#     # Union new and changed rows\n",
    "#     result_df = new_rows.union(changed_rows)\n",
    "\n",
    "#     return result_df\n",
    "\n",
    "# # Example usage\n",
    "# result_df = perform_cdc(\n",
    "#     existing_df,\n",
    "#     target_df\n",
    "# )\n",
    "# result_df.show()\n",
    "# result_df.drop(\"row_hash\").writeTo(\"hello\").overwritePartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c581824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXISTING DATA\n",
      "+---+-----+--------+\n",
      "| id| data|category|\n",
      "+---+-----+--------+\n",
      "|  1|hello|       A|\n",
      "|  2|world|       B|\n",
      "|  3|  foo|       A|\n",
      "+---+-----+--------+\n",
      "\n",
      "new data\n",
      "+---+-----+--------+\n",
      "| id| data|category|\n",
      "+---+-----+--------+\n",
      "|  1|  Lol|       A|\n",
      "|  4|hello|       B|\n",
      "+---+-----+--------+\n",
      "\n",
      "New DATA:\n",
      "+---+-----+--------+--------------------+--------------------+------+\n",
      "| id| data|category|            row_hash|          updated_at|active|\n",
      "+---+-----+--------+--------------------+--------------------+------+\n",
      "|  4|hello|       B|ef9cb88bf62ccf2b7...|2025-07-08 16:16:...|     1|\n",
      "+---+-----+--------+--------------------+--------------------+------+\n",
      "\n",
      "OLD Versions (marked active=0):\n",
      "+---+-----+--------+--------------------+--------------------+------+\n",
      "| id| data|category|            row_hash|          updated_at|active|\n",
      "+---+-----+--------+--------------------+--------------------+------+\n",
      "|  1|hello|       A|01de35f326f8d903c...|2025-07-08 16:16:...|     0|\n",
      "+---+-----+--------+--------------------+--------------------+------+\n",
      "\n",
      "NEW Versions (active=1):\n",
      "+---+----+--------+--------------------+--------------------+------+\n",
      "| id|data|category|            row_hash|          updated_at|active|\n",
      "+---+----+--------+--------------------+--------------------+------+\n",
      "|  1| Lol|       A|bcb799330cad7b94a...|2025-07-08 16:16:...|     1|\n",
      "+---+----+--------+--------------------+--------------------+------+\n",
      "\n",
      "Final Result:\n",
      "+---+-----+--------+--------------------+--------------------+------+\n",
      "| id| data|category|            row_hash|          updated_at|active|\n",
      "+---+-----+--------+--------------------+--------------------+------+\n",
      "|  4|hello|       B|ef9cb88bf62ccf2b7...|2025-07-08 16:16:...|     1|\n",
      "|  1|  Lol|       A|bcb799330cad7b94a...|2025-07-08 16:16:...|     1|\n",
      "|  1|hello|       A|01de35f326f8d903c...|2025-07-08 16:16:...|     0|\n",
      "|  3|  foo|       A|c56e286073c2d81af...|2025-07-08 16:16:...|     1|\n",
      "|  2|world|       B|ea7a2792e7fcaa2c1...|2025-07-08 16:16:...|     1|\n",
      "+---+-----+--------+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# source_df is the new data, target_df is the existing data\n",
    "# IN CDC nomenclature, source is the new data and target is the existing data\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "incoming_data = [\n",
    "    (1, \"Lol\", \"A\"),  # changed \n",
    "    (4, \"hello\", \"B\"), # new\n",
    "]\n",
    "\n",
    "# Read existing data\n",
    "existing_df = spark.read.table(\"mysql.default.hello\")\n",
    "print(\"EXISTING DATA\")\n",
    "existing_df.show()\n",
    "\n",
    "# Create new dataframe\n",
    "new_df = spark.createDataFrame(incoming_data, [\"id\", \"data\", \"category\"])\n",
    "print(\"new data\")\n",
    "new_df.show()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sha2, concat_ws, col, lit, current_timestamp\n",
    "\n",
    "def perform_cdc(existing_df, new_df, primary_key=\"id\"):\n",
    "    # Get columns to hash (exclude metadata columns)\n",
    "    hash_columns = [c for c in existing_df.columns if c not in [\"row_hash\", \"updated_at\", \"active\"]]\n",
    "    \n",
    "    # Add hash columns and timestamp\n",
    "    source_hashed = new_df.withColumn(\"row_hash\", sha2(concat_ws(\"|\", *new_df.columns), 256)) \\\n",
    "                          .withColumn(\"updated_at\", current_timestamp()) \\\n",
    "                          .withColumn(\"active\", lit(1))\n",
    "    \n",
    "    # For existing data, keep the existing hash if columns exist\n",
    "    if \"row_hash\" in existing_df.columns and \"updated_at\" in existing_df.columns and \"active\" in existing_df.columns:\n",
    "        target_hashed = existing_df\n",
    "    else:\n",
    "        target_hashed = existing_df.withColumn(\"row_hash\", sha2(concat_ws(\"|\", *hash_columns), 256)) \\\n",
    "                                   .withColumn(\"updated_at\", current_timestamp()) \\\n",
    "                                   .withColumn(\"active\", lit(1))\n",
    "    \n",
    "    # Find new rows (not in target)\n",
    "    new_rows = source_hashed.join(\n",
    "        target_hashed.select(primary_key),\n",
    "        [primary_key],\n",
    "        \"left_anti\"\n",
    "    )\n",
    "    print(\"New DATA:\")\n",
    "    new_rows.show()\n",
    "    \n",
    "    # Detect changed rows: same ID, but different row_hash\n",
    "    source_alias = source_hashed.alias(\"src\")\n",
    "    target_alias = target_hashed.alias(\"tgt\")\n",
    "\n",
    "    # changed_rows = source_alias.join(\n",
    "    #     target_alias.select(primary_key, \"row_hash\"),\n",
    "    #     on=primary_key,\n",
    "    #     how=\"inner\"\n",
    "    # ).filter(\n",
    "    #     col(\"src.row_hash\") != col(\"tgt.row_hash\")\n",
    "    # ).select(\"src.*\")\n",
    "    \n",
    "    # print(\"Changed DATA:\")\n",
    "    # changed_rows.show()\n",
    "\n",
    "    # Detect changed rows (same ID but different hash)\n",
    "    changed_rows_new = source_hashed.alias(\"src\").join(\n",
    "        target_hashed.alias(\"tgt\").select(primary_key, \"row_hash\"),\n",
    "        primary_key,\n",
    "        \"inner\"\n",
    "    ).filter(\"src.row_hash != tgt.row_hash\").select(\"src.*\")\n",
    "    \n",
    "    # Mark OLD versions of changed rows as inactive (active=0)\n",
    "    changed_rows_old = target_hashed.alias(\"tgt\").join(\n",
    "        changed_rows_new.select(primary_key).alias(\"src\"),\n",
    "        primary_key,\n",
    "        \"inner\"\n",
    "    ).withColumn(\"active\", F.lit(0)) \\\n",
    "     .withColumn(\"updated_at\", F.current_timestamp())\n",
    "    \n",
    "    print(\"OLD Versions (marked active=0):\")\n",
    "    changed_rows_old.show()\n",
    "    print(\"NEW Versions (active=1):\")\n",
    "    changed_rows_new.show()\n",
    "    \n",
    "    # Unchanged rows (remain as-is)\n",
    "    unchanged_rows = target_hashed.join(\n",
    "        source_hashed.select(primary_key),\n",
    "        primary_key,\n",
    "        \"left_anti\"\n",
    "    ).filter(\"active == 1\")  # Keep only active rows\n",
    "\n",
    "    # Combine all: new, updated (new=1 + old=0), and unchanged\n",
    "    result_df = new_rows.union(changed_rows_new) \\\n",
    "                        .union(changed_rows_old) \\\n",
    "                        .union(unchanged_rows)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Perform CDC\n",
    "result_df = perform_cdc(existing_df, new_df)\n",
    "print(\"Final Result:\")\n",
    "result_df.show()\n",
    "\n",
    "# Write back to table\n",
    "# result_df.drop(\"row_hash\").write.mode(\"overwrite\").saveAsTable(\"mysql.default.hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd729211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE hello (\n",
    "id bigint,\n",
    "data string,\n",
    "category string)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b5fc17c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO mysql.default.hello VALUES\n",
    "(1, \"hello\", \"A\"),\n",
    "(2, \"world\", \"B\"),\n",
    "(3, \"foo\", \"A\")\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
